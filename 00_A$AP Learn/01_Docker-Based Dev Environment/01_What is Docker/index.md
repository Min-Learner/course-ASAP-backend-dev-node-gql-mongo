### 2-minute Docker Briefing 

Docker is a daemon process that manages running containers and processes invoked via the Command Line Interface (CLI). Docker Containers are always started from Docker Images. Docker images are like VM images - they have the Operating System and programs installed, and usually some additional files with configuration and data present. Unlike VMs, at runtime, containers pretty much run within the host's Operating System environment, have a User visible on the host and may share file system and ports. There are popular diagrams that show the containers stack up on some common foundation on the host. From the practical standpoint, think of containers as separate little machines using host's hardware and network, and able to communicate with each other, usually via ports on the host, although, they multiple containers may access the same files as well. 

As containers have access to the host, there are obvious security risks of exposing host's or other containers' stuff to malicious containers, just be mindful of that. The proper Docker theory assumes that containers run a single process, ephemeral in nature, and when the purpose of the process is fulfilled, containers are done and over with. In practical reality, though, containers, of course, can be used in any way useful and convenient, just keep in mind that the container's file system by itself is not persisted, and once the container is "removed" - it is gone. If you do want to keep updates to the file system done by the container - you must explicitly use volumes shared with the host when you start (`run`) the container for the first time.

In accordance with the theory, containers must always run some process when they are started - either the one specified in the image that the container is started from or the override provided in the `docker run` command. This is annoying when you just want to start a generic container that runs as a daemon machine waiting for you to access it and try some things out, but there are easy ways to emulate a dummy daemon in the container that would keep it from shutting down.

### The Three Docker Commands 

As container's file system, by definition, is not persisted, anything that you need to preserve and re-use should be packaged as an image. Naturally, you may always save your container as an image, but the proper way of creating images is via `Dockerfile`. The main reason is that in the Infrastructure-as-Code paradigm you should always write down all the installation and configuration steps - and Dockerfile is a perfect place for that. Dockerfile is literally a sequence of commands that configure the image / container. Those are executed when you run `docker build`. In the Dockerfile, you also need to specify the source image (can be multiple - separate for different build steps). `docker build` runs on the host and technically is identical to `docker run` - both start a little machine (container) and execute commands in it, but, somewhat unfortunately, `build` and `run` have their own, not always overlapping options, and may act differently in similar situations. If the `build` errors out, there is no clean way to debug and troubleshoot it, although you can usually see the stopped container that you `commit` as an image and try starting for manual troubleshooting.

So, `docker build` is the complex process (lots fo room for improvement), but the other two commands are very simple: `docker run` creates and kicks of a container from the built image and `docker exec` executes a one-off command in the running container, most often used to simply access its shell prompt and start poking around.

The commands have a long (hugely long) list of parameters that mostly control the host-container coordination like ports, shared volumes, use of networking. Alternatively, `docker-compose` exists to manage the parameters in a YAML file, for a group of containers vs. one at a time. In the end, those are just different flavours of the same tech foundation.
